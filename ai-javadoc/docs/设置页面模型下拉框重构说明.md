# 设置页面模型下拉框重构说明

## 重构概述

本次重构主要针对设置页面的模型下拉框逻辑进行了优化，实现了根据AI提供商的接口动态获取模型列表的功能。

## 主要变更

### 1. 调整输入框顺序

**变更内容：**

- 将设置页面的输入框顺序调整为：AI提供商 → Base URL → API Key → 模型
- 更符合用户配置的逻辑顺序：先选择提供商，再配置连接信息，最后选择模型

**修改文件：**

- `JavaDocSettingsPanel.java` - 调整了FormBuilder中的组件顺序

### 2. 新增模型获取接口

**变更内容：**

- 在 `AIServiceProvider` 接口中新增 `getAvailableModels()` 方法
- 该方法通过调用提供商的API接口获取当前可用的模型列表
- 与现有的 `getSupportedModels()` 方法不同，`getAvailableModels()` 返回实时数据

**接口定义：**

```java
@NotNull
List<String> getAvailableModels();
```

### 3. 实现通用模型获取逻辑

**变更内容：**

- 在 `AICompatibleProvider` 中实现了 `getAvailableModels()` 方法
- 默认实现调用 `{baseUrl}/models` 端点获取模型列表
- 支持标准的OpenAI兼容格式的响应解析
- 包含完整的错误处理机制

**实现特点：**

- 发送GET请求到 `/models` 端点
- 解析JSON响应获取模型列表
- 处理网络异常、认证失败、响应格式错误等情况
- 失败时返回空列表而不是抛出异常

### 4. 为Ollama提供商添加特定支持

**变更内容：**

- 在 `OllamaProvider` 中重写了 `parseModelsResponse()` 方法
- 支持Ollama特定的响应格式（使用 `models` 字段而不是 `data` 字段）
- 优先使用 `name` 字段，如果不存在则使用 `model` 字段

**Ollama响应格式：**

```json
{
  "models": [
    {
      "name": "qwen:7b",
      "model": "qwen:7b",
      "modified_at": "2024-01-01T00:00:00Z",
      "size": 1234567890,
      "digest": "sha256:...",
      "details": { ... }
    }
  ]
}
```

### 5. 更新UI逻辑支持动态加载

**变更内容：**

- 在 `JavaDocSettingsPanel` 中新增 `updateAvailableModels()` 方法
- 当用户输入Base URL和API Key后，自动尝试获取可用模型列表
- 添加延迟机制避免频繁请求
- 在后台线程中执行模型获取，避免阻塞UI

**更新策略：**

- 检查是否已输入必要的配置信息
- 创建临时的提供商实例
- 调用 `getAvailableModels()` 获取实际模型列表
- 更新下拉框内容
- 保持用户当前选择的模型（如果仍然可用）

**监听器优化：**

- Base URL变更时触发模型列表更新
- API Key变更时触发模型列表更新
- 添加500ms延迟避免频繁请求
- 在后台线程中执行，不阻塞UI

## 技术实现细节

### 错误处理策略

1. **网络异常**：记录日志，返回空列表
2. **认证失败**：记录日志，返回空列表
3. **响应格式错误**：记录日志，返回空列表
4. **超时**：记录日志，返回空列表

### 性能优化

1. **延迟请求**：用户停止输入500ms后才发起请求
2. **后台执行**：模型获取在后台线程中执行
3. **静默失败**：获取失败时保持当前模型列表，不干扰用户

### 用户体验优化

1. **保持选择**：尝试保持用户当前选择的模型
2. **智能回退**：如果当前模型不可用，选择第一个可用模型
3. **提示更新**：更新工具提示文本显示模型来源

## 支持的AI提供商

### Ollama

- **API端点**：`GET {baseUrl}/models`
- **响应格式**：使用 `models` 字段
- **模型字段**：优先使用 `name`，备选 `model`
- **特殊处理**：不需要API Key

### OpenAI兼容服务

- **API端点**：`GET {baseUrl}/models`
- **响应格式**：使用 `data` 字段
- **模型字段**：使用 `id` 字段
- **特殊处理**：需要API Key

### 通义千问

- **API端点**：`GET {baseUrl}/models`
- **响应格式**：使用 `data` 字段
- **模型字段**：使用 `id` 字段
- **特殊处理**：需要API Key

## 使用说明

### 用户操作流程

1. **选择AI提供商**：从下拉框中选择提供商（qianwen/ollama/custom）
2. **配置Base URL**：输入API服务地址
3. **配置API Key**：如果需要的话，输入API密钥
4. **选择模型**：系统会自动获取可用模型列表并更新下拉框
5. **测试连接**：点击"测试连接"按钮验证配置

### 模型列表更新时机

- 切换AI提供商时
- 输入Base URL后（延迟500ms）
- 输入API Key后（延迟500ms）

## 向后兼容性

- 保持了原有的 `getSupportedModels()` 方法
- 保持了原有的模型选择逻辑
- 如果动态获取失败，会回退到推荐的模型列表
- 用户仍然可以手动输入任何模型名称

## 未来扩展

1. **缓存机制**：可以考虑添加模型列表缓存，减少重复请求
2. **更多提供商**：可以轻松添加新的AI提供商支持
3. **模型信息**：可以扩展获取更多模型信息（如参数数量、价格等）
4. **批量操作**：可以支持批量获取多个提供商的模型列表

## 测试建议

1. **功能测试**：
    - 测试不同提供商的模型获取
    - 测试网络异常情况
    - 测试API Key无效情况

2. **性能测试**：
    - 测试大量模型时的UI响应
    - 测试网络延迟情况下的用户体验

3. **兼容性测试**：
    - 测试不同版本的Ollama
    - 测试不同的OpenAI兼容服务

